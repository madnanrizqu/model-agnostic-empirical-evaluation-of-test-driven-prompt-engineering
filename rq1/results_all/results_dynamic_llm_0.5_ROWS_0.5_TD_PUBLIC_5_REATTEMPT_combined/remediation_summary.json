{
  "total_comparisons": 16,
  "td_better": 13,
  "td_same": 3,
  "td_worse": 0,
  "using_remediation": true,
  "details": {
    "human_eval_chatgpt4o_combined": {
      "status": "same",
      "base_accuracy": 91.46,
      "td_accuracy": 91.46,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_chatgpt4o_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_chatgpt4o_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_chatgpt4o_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_chatgpt4o_combined_td"
    },
    "human_eval_chatgpt4omini_combined": {
      "status": "same",
      "base_accuracy": 88.41,
      "td_accuracy": 88.41,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_chatgpt4omini_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_chatgpt4omini_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_chatgpt4omini_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_chatgpt4omini_combined_td"
    },
    "human_eval_claude35haiku_combined": {
      "status": "same",
      "base_accuracy": 92.68,
      "td_accuracy": 92.68,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_claude35haiku_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_claude35haiku_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_claude35haiku_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_claude35haiku_combined_td"
    },
    "human_eval_claude35sonnet_combined": {
      "status": "better",
      "base_accuracy": 95.73,
      "td_accuracy": 96.34,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_claude35sonnet_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_claude35sonnet_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_claude35sonnet_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_claude35sonnet_combined_td"
    },
    "human_eval_qwen25coder14b_combined": {
      "status": "better",
      "base_accuracy": 80.49,
      "td_accuracy": 86.59,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder14b_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder14b_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder14b_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder14b_combined_td"
    },
    "human_eval_qwen25coder32b_combined": {
      "status": "better",
      "base_accuracy": 87.8,
      "td_accuracy": 91.46,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder32b_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder32b_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder32b_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder32b_combined_td"
    },
    "human_eval_qwen25coder3b_combined": {
      "status": "better",
      "base_accuracy": 76.22,
      "td_accuracy": 78.66,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder3b_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder3b_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder3b_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder3b_combined_td"
    },
    "human_eval_qwen25coder7b_combined": {
      "status": "better",
      "base_accuracy": 79.27,
      "td_accuracy": 80.49,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder7b_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder7b_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder7b_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/human_eval_qwen25coder7b_combined_td"
    },
    "mbpp_sanitized_chatgpt4o_combined": {
      "status": "better",
      "base_accuracy": 88.52,
      "td_accuracy": 91.1,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_chatgpt4o_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_chatgpt4o_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_chatgpt4o_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_chatgpt4o_combined_td"
    },
    "mbpp_sanitized_chatgpt4omini_combined": {
      "status": "better",
      "base_accuracy": 82.67,
      "td_accuracy": 85.25,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_chatgpt4omini_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_chatgpt4omini_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_chatgpt4omini_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_chatgpt4omini_combined_td"
    },
    "mbpp_sanitized_claude35haiku_combined": {
      "status": "better",
      "base_accuracy": 90.87,
      "td_accuracy": 92.74,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_claude35haiku_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_claude35haiku_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_claude35haiku_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_claude35haiku_combined_td"
    },
    "mbpp_sanitized_claude35sonnet_combined": {
      "status": "better",
      "base_accuracy": 92.97,
      "td_accuracy": 93.68,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_claude35sonnet_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_claude35sonnet_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_claude35sonnet_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_claude35sonnet_combined_td"
    },
    "mbpp_sanitized_qwen25coder14b_combined": {
      "status": "better",
      "base_accuracy": 73.77,
      "td_accuracy": 83.14,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder14b_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder14b_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder14b_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder14b_combined_td"
    },
    "mbpp_sanitized_qwen25coder32b_combined": {
      "status": "better",
      "base_accuracy": 84.78,
      "td_accuracy": 88.06,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder32b_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder32b_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder32b_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder32b_combined_td"
    },
    "mbpp_sanitized_qwen25coder3b_combined": {
      "status": "better",
      "base_accuracy": 63.7,
      "td_accuracy": 68.62,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder3b_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder3b_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder3b_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder3b_combined_td"
    },
    "mbpp_sanitized_qwen25coder7b_combined": {
      "status": "better",
      "base_accuracy": 65.57,
      "td_accuracy": 77.75,
      "using_remediation": true,
      "base_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder7b_combined/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "td_summary_path": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder7b_combined_td/results_CHATGPT_4O_MINI_0.5_ROWS_0.5_TD_PUBLIC_5_REATTEMPT_combined/summary.json",
      "base_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder7b_combined",
      "td_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1/mbpp_sanitized_qwen25coder7b_combined_td"
    }
  },
  "rq_dir": "/Users/madnanrizqu/Code/Research/learn/thesis/experiment_runner/../rq1",
  "results_folder": "dynamic",
  "timestamp": "2025-10-17 22:52:19",
  "incomplete_directories": {
    "missing_td_dirs": [],
    "missing_summary_files": [],
    "missing_accuracy_data": [],
    "partial_completion_dirs": [],
    "successful_comparisons": [
      "human_eval_chatgpt4o_combined",
      "human_eval_chatgpt4omini_combined",
      "human_eval_claude35haiku_combined",
      "human_eval_claude35sonnet_combined",
      "human_eval_qwen25coder14b_combined",
      "human_eval_qwen25coder32b_combined",
      "human_eval_qwen25coder3b_combined",
      "human_eval_qwen25coder7b_combined",
      "mbpp_sanitized_chatgpt4o_combined",
      "mbpp_sanitized_chatgpt4omini_combined",
      "mbpp_sanitized_claude35haiku_combined",
      "mbpp_sanitized_claude35sonnet_combined",
      "mbpp_sanitized_qwen25coder14b_combined",
      "mbpp_sanitized_qwen25coder32b_combined",
      "mbpp_sanitized_qwen25coder3b_combined",
      "mbpp_sanitized_qwen25coder7b_combined"
    ],
    "config_mismatch_dirs": [],
    "total_directories": 16
  },
  "accuracy_statistics": {
    "increases": [
      0.0,
      0.0,
      0.0,
      0.6099999999999994,
      6.1000000000000085,
      3.6599999999999966,
      2.4399999999999977,
      1.2199999999999989,
      2.5799999999999983,
      2.5799999999999983,
      1.8699999999999903,
      0.710000000000008,
      9.370000000000005,
      3.280000000000001,
      4.920000000000002,
      12.180000000000007
    ],
    "total_increase": 51.52000000000001,
    "avg_increase": 3.2200000000000006,
    "median_increase": 2.509999999999998,
    "std_dev": 3.4732616755244172,
    "min_increase": 0.0,
    "max_increase": 12.180000000000007,
    "percentile_25": 0.6850000000000058,
    "percentile_75": 3.974999999999998,
    "interquartile_range": 3.289999999999992,
    "improved_count": 13,
    "worsened_count": 0,
    "same_count": 3,
    "avg_improvement_pct": 5.296262920973656,
    "avg_regression_pct": 0,
    "confidence_interval": [
      1.3692294950237178,
      5.0707705049762835
    ],
    "sorted_increases_desc": [
      {
        "benchmark": "mbpp_sanitized_qwen25coder7b_combined",
        "base_accuracy": 65.57,
        "td_accuracy": 77.75,
        "increase": 12.180000000000007,
        "pct_change": 18.575568095165483
      },
      {
        "benchmark": "mbpp_sanitized_qwen25coder14b_combined",
        "base_accuracy": 73.77,
        "td_accuracy": 83.14,
        "increase": 9.370000000000005,
        "pct_change": 12.701640233157116
      },
      {
        "benchmark": "human_eval_qwen25coder14b_combined",
        "base_accuracy": 80.49,
        "td_accuracy": 86.59,
        "increase": 6.1000000000000085,
        "pct_change": 7.578581190209975
      },
      {
        "benchmark": "mbpp_sanitized_qwen25coder3b_combined",
        "base_accuracy": 63.7,
        "td_accuracy": 68.62,
        "increase": 4.920000000000002,
        "pct_change": 7.723704866562012
      },
      {
        "benchmark": "human_eval_qwen25coder32b_combined",
        "base_accuracy": 87.8,
        "td_accuracy": 91.46,
        "increase": 3.6599999999999966,
        "pct_change": 4.168564920273345
      },
      {
        "benchmark": "mbpp_sanitized_qwen25coder32b_combined",
        "base_accuracy": 84.78,
        "td_accuracy": 88.06,
        "increase": 3.280000000000001,
        "pct_change": 3.8688369898560997
      },
      {
        "benchmark": "mbpp_sanitized_chatgpt4o_combined",
        "base_accuracy": 88.52,
        "td_accuracy": 91.1,
        "increase": 2.5799999999999983,
        "pct_change": 2.9145955716222307
      },
      {
        "benchmark": "mbpp_sanitized_chatgpt4omini_combined",
        "base_accuracy": 82.67,
        "td_accuracy": 85.25,
        "increase": 2.5799999999999983,
        "pct_change": 3.120841901536226
      },
      {
        "benchmark": "human_eval_qwen25coder3b_combined",
        "base_accuracy": 76.22,
        "td_accuracy": 78.66,
        "increase": 2.4399999999999977,
        "pct_change": 3.2012595119391203
      },
      {
        "benchmark": "mbpp_sanitized_claude35haiku_combined",
        "base_accuracy": 90.87,
        "td_accuracy": 92.74,
        "increase": 1.8699999999999903,
        "pct_change": 2.0578848905029057
      },
      {
        "benchmark": "human_eval_qwen25coder7b_combined",
        "base_accuracy": 79.27,
        "td_accuracy": 80.49,
        "increase": 1.2199999999999989,
        "pct_change": 1.5390437744417798
      },
      {
        "benchmark": "mbpp_sanitized_claude35sonnet_combined",
        "base_accuracy": 92.97,
        "td_accuracy": 93.68,
        "increase": 0.710000000000008,
        "pct_change": 0.763687210928265
      },
      {
        "benchmark": "human_eval_claude35sonnet_combined",
        "base_accuracy": 95.73,
        "td_accuracy": 96.34,
        "increase": 0.6099999999999994,
        "pct_change": 0.6372088164629681
      },
      {
        "benchmark": "human_eval_chatgpt4o_combined",
        "base_accuracy": 91.46,
        "td_accuracy": 91.46,
        "increase": 0.0,
        "pct_change": 0.0
      },
      {
        "benchmark": "human_eval_chatgpt4omini_combined",
        "base_accuracy": 88.41,
        "td_accuracy": 88.41,
        "increase": 0.0,
        "pct_change": 0.0
      },
      {
        "benchmark": "human_eval_claude35haiku_combined",
        "base_accuracy": 92.68,
        "td_accuracy": 92.68,
        "increase": 0.0,
        "pct_change": 0.0
      }
    ],
    "sorted_regressions_asc": [
      {
        "benchmark": "human_eval_chatgpt4o_combined",
        "base_accuracy": 91.46,
        "td_accuracy": 91.46,
        "increase": 0.0,
        "pct_change": 0.0
      },
      {
        "benchmark": "human_eval_chatgpt4omini_combined",
        "base_accuracy": 88.41,
        "td_accuracy": 88.41,
        "increase": 0.0,
        "pct_change": 0.0
      },
      {
        "benchmark": "human_eval_claude35haiku_combined",
        "base_accuracy": 92.68,
        "td_accuracy": 92.68,
        "increase": 0.0,
        "pct_change": 0.0
      },
      {
        "benchmark": "human_eval_claude35sonnet_combined",
        "base_accuracy": 95.73,
        "td_accuracy": 96.34,
        "increase": 0.6099999999999994,
        "pct_change": 0.6372088164629681
      },
      {
        "benchmark": "mbpp_sanitized_claude35sonnet_combined",
        "base_accuracy": 92.97,
        "td_accuracy": 93.68,
        "increase": 0.710000000000008,
        "pct_change": 0.763687210928265
      },
      {
        "benchmark": "human_eval_qwen25coder7b_combined",
        "base_accuracy": 79.27,
        "td_accuracy": 80.49,
        "increase": 1.2199999999999989,
        "pct_change": 1.5390437744417798
      },
      {
        "benchmark": "mbpp_sanitized_claude35haiku_combined",
        "base_accuracy": 90.87,
        "td_accuracy": 92.74,
        "increase": 1.8699999999999903,
        "pct_change": 2.0578848905029057
      },
      {
        "benchmark": "human_eval_qwen25coder3b_combined",
        "base_accuracy": 76.22,
        "td_accuracy": 78.66,
        "increase": 2.4399999999999977,
        "pct_change": 3.2012595119391203
      },
      {
        "benchmark": "mbpp_sanitized_chatgpt4o_combined",
        "base_accuracy": 88.52,
        "td_accuracy": 91.1,
        "increase": 2.5799999999999983,
        "pct_change": 2.9145955716222307
      },
      {
        "benchmark": "mbpp_sanitized_chatgpt4omini_combined",
        "base_accuracy": 82.67,
        "td_accuracy": 85.25,
        "increase": 2.5799999999999983,
        "pct_change": 3.120841901536226
      },
      {
        "benchmark": "mbpp_sanitized_qwen25coder32b_combined",
        "base_accuracy": 84.78,
        "td_accuracy": 88.06,
        "increase": 3.280000000000001,
        "pct_change": 3.8688369898560997
      },
      {
        "benchmark": "human_eval_qwen25coder32b_combined",
        "base_accuracy": 87.8,
        "td_accuracy": 91.46,
        "increase": 3.6599999999999966,
        "pct_change": 4.168564920273345
      },
      {
        "benchmark": "mbpp_sanitized_qwen25coder3b_combined",
        "base_accuracy": 63.7,
        "td_accuracy": 68.62,
        "increase": 4.920000000000002,
        "pct_change": 7.723704866562012
      },
      {
        "benchmark": "human_eval_qwen25coder14b_combined",
        "base_accuracy": 80.49,
        "td_accuracy": 86.59,
        "increase": 6.1000000000000085,
        "pct_change": 7.578581190209975
      },
      {
        "benchmark": "mbpp_sanitized_qwen25coder14b_combined",
        "base_accuracy": 73.77,
        "td_accuracy": 83.14,
        "increase": 9.370000000000005,
        "pct_change": 12.701640233157116
      },
      {
        "benchmark": "mbpp_sanitized_qwen25coder7b_combined",
        "base_accuracy": 65.57,
        "td_accuracy": 77.75,
        "increase": 12.180000000000007,
        "pct_change": 18.575568095165483
      }
    ],
    "normality_test_stat": 0.835265069009981,
    "normality_p_value": 0.008331700773165874,
    "is_normal": false,
    "significance_test_type": "wilcoxon_signed_rank",
    "significance_test_stat": 0.0,
    "significance_p_value": 0.0014688499662158369,
    "cohens_d": 0.3753593587470389,
    "effect_size_interpretation": "small"
  }
}